{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b21798e-52a0-44bc-a016-09822b5bc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchattacks\n",
    "from torchattacks import OnePixel, Pixle, DeepFool, CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0e5029-5291-4745-a014-7753f20a0cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qaiser/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/qaiser/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = models.efficientnet_b0(pretrained=True)\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_weights_path = \"models/efficientNetB0_binary_ucf_crime.pth\"\n",
    "\n",
    "model_ft.load_state_dict(torch.load(model_weights_path, map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "test_loader = dataloaders[\"test\"]\n",
    "\n",
    "model = model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2919514d-c42c-480c-bb85-fc4220ed4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf2850c-2666-4bf8-9508-9dc9fa60ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def save_rgb(adv_images, filename):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    denormalize = transforms.Normalize(\n",
    "        mean=[-m/s for m, s in zip(mean, std)],\n",
    "        std=[1/s for s in std]\n",
    "    )\n",
    "    \n",
    "    adv_images_denorm = adv_images.clone()\n",
    "    \n",
    "    for i in range(adv_images_denorm.size(0)):\n",
    "        adv_images_denorm[i] = denormalize(adv_images_denorm[i])\n",
    "    \n",
    "    adv_images_denorm = torch.clamp(adv_images_denorm, 0, 1)\n",
    "\n",
    "    ### new code start ###\n",
    "    for i, adv_image in enumerate(adv_images_denorm):\n",
    "        # Convert the tensor to an image format\n",
    "        adv_image_rgb = transforms.ToPILImage()(adv_image.cpu())\n",
    "        \n",
    "        # Save the image\n",
    "        adv_image_rgb.save(f\"adversarial_image_{i}.png\")\n",
    "    ### new code end ###\n",
    "    \n",
    "    vutils.save_image(adv_images_denorm, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c06a810-7c6d-4f70-bccb-654c4a22ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixle = torchattacks.Pixle(model,\n",
    "        x_dimensions=(1, 2),\n",
    "        y_dimensions=(1, 2),\n",
    "        pixel_mapping=\"random\",\n",
    "        restarts=5,\n",
    "        max_iterations=10,\n",
    "        update_each_iteration=False,)\n",
    "\n",
    "onepixel = torchattacks.OnePixel(\n",
    "    model, \n",
    "    pixels=5,        # Start with a single pixel for minimal perturbation\n",
    "    steps=30,        # Increase the steps for better optimization\n",
    "    popsize=34,      # Increase population size for better candidate exploration\n",
    "    inf_batch=4    # Standard batch size, can be increased if memory allows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d45362-eb2f-4d70-8288-bf9ab61949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = torchattacks.MultiAttack([pixle, onepixel])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cee84b-74c9-43b5-b4a2-ce8717118fe2",
   "metadata": {},
   "source": [
    "### One Pixel Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1cfdc7-ed1b-44e7-8ec3-48c12454188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images = onepixel(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457f56b9-b5e2-4684-b1f5-598e3ccb5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rgb(adv_images, \"RGB_onepixel_adversarial_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabdddf-8e16-41e8-bb36-65403affeb6e",
   "metadata": {},
   "source": [
    "### Pixle Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b430d3f-6d99-48ed-8b78-5ae3002ba465",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images_pixle = pixle(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378d651e-3ee1-401b-ae11-7d4c27d129d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rgb(adv_images_pixle, \"RGB_pixle_adversarial_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebac28e-1b4f-4c20-8d8b-5d41e31314ce",
   "metadata": {},
   "source": [
    "### MPD Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc25d67-a2c4-412b-9a51-ae787af4dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images_mpd = mpd(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e620938-1846-4330-8c95-d1e473a11505",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rgb(adv_images_mpd, \"RGB_mpd_adversarial_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e11e0-5fe2-4e99-ad08-31d260d162aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
